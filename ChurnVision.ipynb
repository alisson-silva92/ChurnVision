{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f65ec8-9f1c-429b-938f-8d16cee4e0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Arquivo final salvo em: C:\\Users\\alisson.silva\\D\\audaces\\BI\\__Bi_Comercial\\previsao_churn\\clientes_risco_final.csv\n",
      "📦 Linhas: 11,460 | Colunas: 16\n",
      "\n",
      "🔧 ML habilitado\n",
      "• SMOTE usado? Sim\n",
      "• Threshold ótimo (prioriza recall / F2): 0.580\n",
      "• Top thresholds (holdout):\n",
      " thr  precision   recall    fbeta  accuracy  tp   fp  fn    tn\n",
      "0.26   0.037673 0.947368 0.162514  0.898143  90 2299   5 20226\n",
      "0.24   0.035629 0.947368 0.154852  0.892087  90 2436   5 20089\n",
      "0.22   0.033545 0.947368 0.146915  0.885146  90 2593   5 19932\n",
      "0.20   0.031870 0.947368 0.140449  0.878912  90 2734   5 19791\n",
      "0.28   0.040308 0.936842 0.171947  0.906057  89 2119   6 20406\n",
      "0.30   0.043435 0.915789 0.182543  0.914943  87 1916   8 20609\n",
      "0.32   0.046943 0.905263 0.194394  0.922414  86 1746   9 20779\n",
      "0.34   0.049675 0.884211 0.202801  0.928470  84 1607  11 20918\n",
      "\n",
      "📊 Resultados (pós-threshold e pós-pipeline):\n",
      "Acurácia: 91.44%\n",
      "Precisão: 60.71%\n",
      "Recall (sensibilidade): 46.38%\n",
      "F1-Score: 52.59%\n",
      "\n",
      "Matriz de Confusão:\n",
      "Verdadeiros Negativos: 9935\n",
      "Falsos Positivos: 352\n",
      "Falsos Negativos: 629\n",
      "Verdadeiros Positivos: 544\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "pasta = r\"C:\\Users\\alisson.silva\\D\\audaces\\BI\\__Bi_Comercial\\previsao_churn\"\n",
    "\n",
    "# Sinais\n",
    "N_RECENT = 3\n",
    "N_BASE   = 6\n",
    "CAP_DIAS = 180\n",
    "GAMMA    = 0.7\n",
    "W_MOMENTO, W_TEND, W_REC = 0.50, 0.35, 0.15\n",
    "ALPHA = 0.5               \n",
    "INFLUENCIA_TICKET = 0.15    \n",
    "W_NPS = 0.15               \n",
    "NEUTRO_NPS = 0.30          \n",
    "\n",
    "\n",
    "K_PRAZO_MESES = 2\n",
    "TARGET_RECALL = 0.40         \n",
    "THR_GRID = np.linspace(0.20, 0.60, 21) \n",
    "\n",
    "# Data \"de hoje\" determinística (timezone do usuário)\n",
    "tz = pytz.timezone(\"Europe/Madrid\")\n",
    "today = pd.Timestamp(datetime.now(tz).date())\n",
    "\n",
    "# Utils\n",
    "\n",
    "def classificar_risco_cont(v: float) -> str:\n",
    "    if v < 0.3: return \"Baixo Risco\"\n",
    "    if v < 0.7: return \"Risco Médio\"\n",
    "    return \"Alto Risco\"\n",
    "\n",
    "def classificar_eng(v: float) -> str:\n",
    "    if v < 0.33: return \"Baixo Engajamento\"\n",
    "    if v < 0.66: return \"Engajamento Médio\"\n",
    "    return \"Alto Engajamento\"\n",
    "\n",
    "def risco_nps(n):\n",
    "    if pd.isna(n): return np.nan\n",
    "    n = float(n)\n",
    "    if n >= 9:   return 0.0   \n",
    "    if n >= 7:   return 0.20  \n",
    "    return 0.60                \n",
    "\n",
    "\n",
    "def peso_ticket(v):\n",
    "    if v < 1000:  return 1\n",
    "    if v < 2500:  return 2\n",
    "    if v < 4000:  return 3\n",
    "    return 4\n",
    "\n",
    "def fbeta_score_from_pr(precision: float, recall: float, beta: float = 2.0) -> float:\n",
    "    \"\"\"Calcula F-beta a partir de P e R.\"\"\"\n",
    "    beta2 = beta**2\n",
    "    denom = (beta2 * precision + recall)\n",
    "    if denom == 0: return 0.0\n",
    "    return (1 + beta2) * (precision * recall) / (denom + 1e-12)\n",
    "\n",
    "def pick_threshold(probas: np.ndarray, y_true: np.ndarray,\n",
    "                   thr_grid: np.ndarray = THR_GRID,\n",
    "                   target_recall: float = TARGET_RECALL,\n",
    "                   beta: float = 2.0) -> Tuple[float, pd.DataFrame]:\n",
    "    \"\"\"Escolhe threshold priorizando recall (via F-beta alto e meta de recall).\"\"\"\n",
    "    rows = []\n",
    "    for thr in thr_grid:\n",
    "        pred = (probas >= thr).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, pred).ravel()\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        fbeta     = fbeta_score_from_pr(precision, recall, beta=beta)\n",
    "        acc       = (tp + tn) / (tp + tn + fp + fn)\n",
    "        rows.append({\"thr\":thr, \"precision\":precision, \"recall\":recall, \"fbeta\":fbeta, \"accuracy\":acc,\n",
    "                     \"tp\":tp, \"fp\":fp, \"fn\":fn, \"tn\":tn})\n",
    "    df_res = pd.DataFrame(rows).sort_values([\"recall\",\"fbeta\",\"precision\"], ascending=[False,False,False])\n",
    "\n",
    "    # 1) tenta cumprir a meta de recall com maior fbeta\n",
    "    cand = df_res[df_res[\"recall\"] >= target_recall]\n",
    "    if not cand.empty:\n",
    "        best = cand.sort_values([\"fbeta\",\"precision\"], ascending=[False,False]).iloc[0]\n",
    "        return float(best[\"thr\"]), df_res\n",
    "\n",
    "    # 2) se não cumprir meta, pega o maior fbeta geral\n",
    "    best = df_res.sort_values([\"fbeta\",\"recall\"], ascending=[False,False]).iloc[0]\n",
    "    return float(best[\"thr\"]), df_res\n",
    "\n",
    "# 1) Ler base e normalizar\n",
    "\n",
    "def ler_base_unificada(pasta: str) -> pd.DataFrame:\n",
    "    arquivos = glob.glob(os.path.join(pasta, \"*.xlsx\"))\n",
    "    if not arquivos:\n",
    "        raise FileNotFoundError(\"Nenhum .xlsx encontrado na pasta base.\")\n",
    "\n",
    "    df = pd.concat([pd.read_excel(a) for a in arquivos], ignore_index=True)\n",
    "\n",
    "    # Tipos/limpeza\n",
    "    for c in ['Software','Package','Country','CardCode','CardName','State','email']:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "    df['created']   = pd.to_datetime(df.get('created'), errors='coerce')\n",
    "    df['Start']     = pd.to_datetime(df.get('Start'),   errors='coerce')\n",
    "    df['Contagem']  = pd.to_numeric(df.get('Contagem'), errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    df['ano'] = pd.to_numeric(df.get('ano'), errors='coerce').astype('Int64')\n",
    "    df['mes'] = pd.to_numeric(df.get('mes'), errors='coerce').astype('Int64')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# 2) Painel mensal completo (corrigido e robusto)\n",
    "\n",
    "def painel_mensal(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    base = (\n",
    "        df.dropna(subset=['ano','mes'])\n",
    "          .assign(\n",
    "              ano=lambda x: x['ano'].astype(int),\n",
    "              mes=lambda x: x['mes'].astype(int),\n",
    "              Contagem=lambda x: pd.to_numeric(x['Contagem'], errors='coerce').fillna(0)\n",
    "          )\n",
    "          .loc[:, ['CardCode','ano','mes','Contagem']]\n",
    "    )\n",
    "\n",
    "    if base.empty:\n",
    "        raise ValueError(\"Base mensal está vazia após limpeza de ano/mes/Contagem.\")\n",
    "\n",
    "    base['period'] = pd.PeriodIndex(year=base['ano'], month=base['mes'], freq='M')\n",
    "\n",
    "    # Agrega por CardCode x period\n",
    "    agg = (\n",
    "        base.groupby(['CardCode','period'], as_index=False)['Contagem']\n",
    "            .sum()\n",
    "            .sort_values(['CardCode','period'])\n",
    "    )\n",
    "\n",
    "\n",
    "    out_rows = []\n",
    "    for card, g in agg.groupby('CardCode'):\n",
    "        pmin, pmax = g['period'].min(), g['period'].max()\n",
    "        full_p = pd.period_range(pmin, pmax, freq='M')\n",
    "        gi = (\n",
    "            g.set_index('period')[['Contagem']]\n",
    "             .reindex(full_p)\n",
    "             .fillna({'Contagem': 0})\n",
    "             .rename_axis('period')\n",
    "             .reset_index()\n",
    "        )\n",
    "        gi['CardCode'] = card\n",
    "        out_rows.append(gi)\n",
    "\n",
    "    full = pd.concat(out_rows, ignore_index=True)\n",
    "\n",
    "  \n",
    "    ts = full['period'].dt.to_timestamp(how='start')\n",
    "    full['ano'] = ts.dt.year.astype(int)\n",
    "    full['mes'] = ts.dt.month.astype(int)\n",
    "    full['dt']  = pd.to_datetime(full['ano'].astype(str) + '-' + full['mes'].astype(str) + '-01')\n",
    "\n",
    "    full = full[['CardCode','ano','mes','Contagem','dt']].sort_values(['CardCode','dt']).reset_index(drop=True)\n",
    "\n",
    "    if full.empty:\n",
    "        raise ValueError(\"painel_mensal gerou vazio — verifique colunas CardCode/ano/mes/Contagem.\")\n",
    "    return full\n",
    "\n",
    "# 3) Estatísticas por cliente\n",
    "\n",
    "def resumo_clientes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dfc = df.groupby('CardCode').agg({\n",
    "        'Contagem': 'sum',\n",
    "        'created':  ['min','max','nunique']\n",
    "    }).reset_index()\n",
    "    dfc.columns = ['CardCode','total_acessos','primeiro_acesso','ultimo_acesso','dias_com_acesso']\n",
    "    dfc['tempo_ativo'] = (dfc['ultimo_acesso'] - dfc['primeiro_acesso']).dt.days.clip(lower=0)\n",
    "\n",
    "    # Recência (dias desde último acesso)\n",
    "    dias = (today - dfc['ultimo_acesso']).dt.days\n",
    "    if dias.notna().any():\n",
    "        max_fallback = dias.dropna().max()\n",
    "    else:\n",
    "        max_fallback = CAP_DIAS\n",
    "    dfc['dias_desde_ultimo'] = dias.fillna(max_fallback).clip(lower=0)\n",
    "\n",
    " \n",
    "    for col in ['total_acessos','dias_com_acesso','tempo_ativo']:\n",
    "        q95 = dfc[col].quantile(0.95)\n",
    "        dfc[col] = dfc[col].clip(upper=q95)\n",
    "\n",
    "    sc_eng = MinMaxScaler()\n",
    "    dfc[['acessos_norm','dias_norm','tempo_norm']] = sc_eng.fit_transform(\n",
    "        dfc[['total_acessos','dias_com_acesso','tempo_ativo']].astype(float)\n",
    "    )\n",
    "    dfc['engajamento_score'] = (\n",
    "        0.5*dfc['acessos_norm'] + 0.3*dfc['dias_norm'] + 0.2*dfc['tempo_norm']\n",
    "    ).clip(0,1)\n",
    "\n",
    "    return dfc\n",
    "\n",
    "# 4) Tendência e queda recentes\n",
    "\n",
    "def slope_stats(g: pd.DataFrame):\n",
    "    g = g.sort_values(['ano','mes'])\n",
    "    x = (g['ano']*12 + g['mes']).astype(float).to_numpy()\n",
    "    y = g['Contagem'].astype(float).to_numpy()\n",
    "\n",
    "    mean_y = np.mean(y)\n",
    "    std_y  = np.std(y, ddof=1) if len(y) > 1 else 0.0\n",
    "\n",
    "    if len(x) < 2 or np.all(y == y[0]):\n",
    "        slope_abs = 0.0\n",
    "    else:\n",
    "        slope_abs = float(linregress(x, y).slope)\n",
    "\n",
    "    eps = 1e-6\n",
    "    slope_per_mean = slope_abs / (mean_y + eps)\n",
    "    slope_per_std  = slope_abs / (std_y  + eps)\n",
    "\n",
    "    return pd.Series({\n",
    "        'media_cliente': mean_y,\n",
    "        'std_cliente': std_y,\n",
    "        'slope_abs': slope_abs,\n",
    "        'slope_per_mean': slope_per_mean,\n",
    "        'slope_per_std': slope_per_std\n",
    "    })\n",
    "\n",
    "def recent_vs_baseline(g: pd.DataFrame):\n",
    "    g = g.sort_values(['ano','mes'])\n",
    "    y = g['Contagem'].astype(float).to_numpy()\n",
    "    if len(y) < (N_RECENT + N_BASE):\n",
    "        return pd.Series({'recent_mean': np.nan, 'baseline_mean': np.nan, 'queda_perc': np.nan})\n",
    "    recent = float(np.mean(y[-N_RECENT:]))\n",
    "    base   = float(np.mean(y[-(N_RECENT+N_BASE):-N_RECENT]))\n",
    "    eps = 1e-6\n",
    "    queda = (base - recent) / (base + eps)\n",
    "    return pd.Series({'recent_mean': recent, 'baseline_mean': base, 'queda_perc': queda})\n",
    "\n",
    "\n",
    "# 5) Ticket (opcional) e NPS (opcional)\n",
    "\n",
    "def ler_ticket(pasta: str) -> pd.DataFrame:\n",
    "    caminho = os.path.join(pasta, \"tkt_medio\", \"ticket_medio.csv\")\n",
    "    if not os.path.exists(caminho):\n",
    "        return pd.DataFrame(columns=['CardCode','TicketMedio','peso_ticket','peso_norm'])\n",
    "    try:\n",
    "        try:\n",
    "            df = pd.read_csv(caminho)\n",
    "        except Exception:\n",
    "            df = pd.read_csv(caminho, sep=';', decimal=',', quoting=csv.QUOTE_MINIMAL)\n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "        df['CardCode'] = df['CardCode'].astype(str).str.strip()\n",
    "        df['TicketMedio'] = pd.to_numeric(df['TicketMedio'], errors='coerce').fillna(0)\n",
    "        df['peso_ticket'] = df['TicketMedio'].apply(peso_ticket)\n",
    "        df['peso_norm'] = ((df['peso_ticket'] - 1) / 3).fillna(0.0)\n",
    "        return df[['CardCode','TicketMedio','peso_ticket','peso_norm']]\n",
    "    except Exception:\n",
    "        return pd.DataFrame(columns=['CardCode','TicketMedio','peso_ticket','peso_norm'])\n",
    "\n",
    "def ler_nps(pasta: str) -> pd.DataFrame:\n",
    "    caminho = os.path.join(pasta, \"NPS.xlsx\")\n",
    "    if not os.path.exists(caminho):\n",
    "        return pd.DataFrame(columns=['CardCode','nota_nps','risco_nps'])\n",
    "    try:\n",
    "        df = pd.read_excel(caminho)\n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "        df = df.rename(columns={'NPS':'nota_nps'})\n",
    "        df['CardCode'] = df['CardCode'].astype(str).str.strip()  # <<< corrigido\n",
    "        df['nota_nps'] = pd.to_numeric(df['nota_nps'], errors='coerce')\n",
    "        df['risco_nps'] = df['nota_nps'].apply(risco_nps)\n",
    "        return df[['CardCode','nota_nps','risco_nps']]\n",
    "    except Exception:\n",
    "        return pd.DataFrame(columns=['CardCode','nota_nps','risco_nps'])\n",
    "\n",
    "# 6) Score por regra (online)\n",
    "\n",
    "def score_regra(df_panel: pd.DataFrame, dfc: pd.DataFrame, df_ticket: pd.DataFrame, df_nps: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_panel is None or df_panel.empty:\n",
    "        raise ValueError(\"score_regra recebeu df_panel vazio.\")\n",
    "\n",
    "    # Tendência\n",
    "    trend = df_panel.groupby('CardCode', as_index=False).apply(slope_stats)\n",
    "    rb    = df_panel.groupby('CardCode', as_index=False).apply(recent_vs_baseline)\n",
    "\n",
    "    base = (dfc.merge(trend, on='CardCode', how='left')\n",
    "              .merge(rb, on='CardCode', how='left'))\n",
    "\n",
    "   \n",
    "    base['queda_perc'] = base['queda_perc'].fillna(0).clip(-1, 1)\n",
    "    base['slope_per_mean'] = base['slope_per_mean'].fillna(0)\n",
    "\n",
    "    base['momento_queda'] = base['queda_perc'].clip(0, 1)\n",
    "    base['tend_rel_inv']  = (-base['slope_per_mean']).clip(lower=0)\n",
    "\n",
    "  \n",
    "    base['recencia_raw'] = base['dias_desde_ultimo'].astype(float).clip(0, CAP_DIAS)\n",
    "    base['recencia_n']   = (base['recencia_raw'] / CAP_DIAS).clip(0, 1)\n",
    "\n",
    "    \n",
    "    _sc = MinMaxScaler()\n",
    "    base[['momento_queda_n','tend_rel_inv_n']] = _sc.fit_transform(\n",
    "        base[['momento_queda','tend_rel_inv']].to_numpy().astype(float)\n",
    "    )\n",
    "\n",
    "    \n",
    "    base['momento_queda_n_b'] = np.power(base['momento_queda_n'], GAMMA)\n",
    "    base['tend_rel_inv_n_b']  = np.power(base['tend_rel_inv_n'],  GAMMA)\n",
    "    base['recencia_n_b']      = np.power(base['recencia_n'],      GAMMA)\n",
    "\n",
    "   \n",
    "    base['risco_raw'] = (\n",
    "        W_MOMENTO*base['momento_queda_n_b'] +\n",
    "        W_TEND   *base['tend_rel_inv_n_b']  +\n",
    "        W_REC    *base['recencia_n_b']\n",
    "    ).clip(0,1)\n",
    "\n",
    "    \n",
    "    rank_pct = base['risco_raw'].rank(pct=True)\n",
    "    rank_pct_boost = np.power(rank_pct, 0.85)\n",
    "    base['risco_blend'] = (ALPHA*base['risco_raw'] + (1-ALPHA)*rank_pct_boost).clip(0,1)\n",
    "\n",
    "    if not df_ticket.empty:\n",
    "        base = base.merge(df_ticket[['CardCode','peso_norm']], on='CardCode', how='left')\n",
    "        base['peso_norm'] = base['peso_norm'].fillna(0.0)\n",
    "    else:\n",
    "        base['peso_norm'] = 0.0\n",
    "\n",
    "    base['risco_churn_novo'] = (base['risco_blend'] + INFLUENCIA_TICKET*base['peso_norm']).clip(0,1)\n",
    "\n",
    "    \n",
    "    base['nivel_engajamento'] = base['engajamento_score'].apply(classificar_eng)\n",
    "\n",
    "    \n",
    "    if not df_nps.empty:\n",
    "        base = base.merge(df_nps[['CardCode','nota_nps','risco_nps']], on='CardCode', how='left')\n",
    "    else:\n",
    "        base['nota_nps'] = np.nan\n",
    "        base['risco_nps'] = np.nan\n",
    "\n",
    "    \n",
    "    base[['media_cliente','std_cliente','momento_queda','tend_rel_inv','recencia_n']] = \\\n",
    "        base[['media_cliente','std_cliente','momento_queda','tend_rel_inv','recencia_n']].fillna(0.0)\n",
    "\n",
    "    base['coef_var'] = (base['std_cliente'] / (base['media_cliente'] + 1e-6)).fillna(0.0)\n",
    "    base['tempo_ativo_meses'] = (base.get('tempo_ativo', pd.Series(0, index=base.index)).astype(float).fillna(0.0) / 30.0)\n",
    "    base['queda_intensa'] = (base['momento_queda'] * (base['baseline_mean'].replace(0, np.nan).fillna(0) > 0).astype(float)).fillna(0.0)\n",
    "\n",
    "    base['nps_risco_fill'] = base['risco_nps'].fillna(NEUTRO_NPS)\n",
    "    base['engaj_score']    = base.get('engajamento_score', pd.Series(0.0, index=base.index)).fillna(0.0)\n",
    "\n",
    "    \n",
    "    base['risco_churn_final_regra'] = (\n",
    "        (1 - W_NPS)*base['risco_churn_novo'] + W_NPS*base['nps_risco_fill']\n",
    "    ).clip(0,1)\n",
    "\n",
    "    base['nivel_risco_regra'] = base['risco_churn_final_regra'].apply(classificar_risco_cont)\n",
    "    base['categoria_churn_regra'] = base['nivel_risco_regra'] + \" / \" + base['nivel_engajamento']\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "# 7) Backtest + ML (melhorado)\n",
    "\n",
    "def tentar_treinar_modelo(df_panel: pd.DataFrame, df_regra: pd.DataFrame, pasta: str) -> Optional[Pipeline]:\n",
    "    \"\"\"Treina um modelo supervisionado com features enriquecidas e tuning de limiar.\n",
    "       Retorna pipeline com atributos:\n",
    "         - thr_opt_ : threshold ótimo (prioriza recall/F2)\n",
    "         - feature_names_ : lista de features\n",
    "         - df_thr_cv_ : tabela com desempenho por threshold no holdout/val\n",
    "    \"\"\"\n",
    "    # Ler cancelamentos\n",
    "    arq_cancel = os.path.join(pasta, \"cancelados.xlsx\")\n",
    "    if not os.path.exists(arq_cancel):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df_cancel = pd.read_excel(arq_cancel)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    df_cancel.columns = [c.strip() for c in df_cancel.columns]\n",
    "    if 'CardCode' not in df_cancel.columns:\n",
    "        return None\n",
    "\n",
    "    df_cancel['CardCode'] = df_cancel['CardCode'].astype(str).str.strip()\n",
    "    df_cancel['dt_cancelamento'] = pd.to_datetime(df_cancel.get('dt_cancelamento'), dayfirst=True, errors='coerce')\n",
    "    df_cancel['dt_aquisicao']    = pd.to_datetime(df_cancel.get('dt_aquisicao'),    dayfirst=True, errors='coerce')\n",
    "\n",
    "    df_cancel_min = (\n",
    "        df_cancel.sort_values('dt_cancelamento')\n",
    "                 .groupby('CardCode', as_index=False)\n",
    "                 .agg(dt_cancelamento=('dt_cancelamento','min'),\n",
    "                      dt_aquisicao=('dt_aquisicao','min'))\n",
    "    )\n",
    "\n",
    "   \n",
    "    map_dt_cancel = dict(zip(df_cancel_min['CardCode'], df_cancel_min['dt_cancelamento']))\n",
    "    map_dt_aq     = dict(zip(df_cancel_min['CardCode'], df_cancel_min['dt_aquisicao']))\n",
    "\n",
    "    \n",
    "    map_peso_norm  = dict(zip(df_regra['CardCode'], df_regra.get('peso_norm', pd.Series(0.0, index=df_regra.index)).fillna(0.0)))\n",
    "    map_nps_risco  = dict(zip(df_regra['CardCode'], df_regra.get('nps_risco_fill', pd.Series(NEUTRO_NPS, index=df_regra.index)).fillna(NEUTRO_NPS)))\n",
    "    map_engaj      = dict(zip(df_regra['CardCode'], df_regra.get('engaj_score', pd.Series(0.0, index=df_regra.index)).fillna(0.0)))\n",
    "\n",
    "    \n",
    "    rows = []\n",
    "    first_dt_map = df_panel.groupby('CardCode')['dt'].min().to_dict()\n",
    "\n",
    "    for card, g in df_panel.groupby('CardCode'):\n",
    "        g = g.sort_values('dt').reset_index(drop=True)\n",
    "        y = g['Contagem'].astype(float).to_numpy()\n",
    "\n",
    "        for i in range(len(g)):\n",
    "            if i+1 < (N_BASE + N_RECENT):\n",
    "                continue\n",
    "\n",
    "            dt_t = g.loc[i, 'dt']\n",
    "\n",
    "            dt_aq = map_dt_aq.get(card, None)\n",
    "            if pd.notna(dt_aq) and dt_t < (dt_aq.replace(day=1)):\n",
    "                continue\n",
    "\n",
    "            xhist = g.iloc[:i+1]\n",
    "            yhist = y[:i+1]\n",
    "\n",
    "           \n",
    "            xx = (xhist['dt'].dt.year*12 + xhist['dt'].dt.month).astype(float).to_numpy()\n",
    "            mean_y = float(np.mean(yhist))\n",
    "            std_y  = float(np.std(yhist, ddof=1)) if len(yhist) > 1 else 0.0\n",
    "            if len(xx) < 2 or np.all(yhist == yhist[0]):\n",
    "                slope_abs = 0.0\n",
    "            else:\n",
    "                slope_abs = float(linregress(xx, yhist).slope)\n",
    "            eps = 1e-6\n",
    "            slope_per_mean = slope_abs / (mean_y + eps)\n",
    "\n",
    "           \n",
    "            if len(yhist) < (N_RECENT + N_BASE):\n",
    "                recent = basev = np.nan\n",
    "                queda = np.nan\n",
    "            else:\n",
    "                recent = float(np.mean(yhist[-N_RECENT:]))\n",
    "                basev  = float(np.mean(yhist[-(N_RECENT+N_BASE):-N_RECENT]))\n",
    "                queda  = (basev - recent) / (basev + eps)\n",
    "            queda = 0 if np.isnan(queda) else float(np.clip(queda, -1, 1))\n",
    "\n",
    "            momento_queda = float(np.clip(queda, 0, 1))\n",
    "            tend_rel_inv  = float(max(-slope_per_mean, 0))\n",
    "\n",
    "            \n",
    "            idx_pos = np.where(np.array(yhist) > 0)[0]\n",
    "            if len(idx_pos) == 0:\n",
    "                meses_sem_uso = np.inf\n",
    "            else:\n",
    "                meses_sem_uso = max((len(yhist) - 1) - idx_pos[-1], 0)\n",
    "            dias_sem_uso  = 30 * (meses_sem_uso if np.isfinite(meses_sem_uso) else CAP_DIAS)\n",
    "            recencia_raw  = float(np.clip(dias_sem_uso, 0, CAP_DIAS))\n",
    "            recencia_n    = float(np.clip(recencia_raw / CAP_DIAS, 0, 1))\n",
    "\n",
    "            \n",
    "            coef_var = (std_y / (mean_y + 1e-6)) if mean_y > 0 else 0.0\n",
    "            tempo_ativo_meses = max((dt_t - first_dt_map.get(card, dt_t)).days / 30.0, 0.0)\n",
    "            queda_intensa = momento_queda * (1.0 if basev > 0 else 0.0)\n",
    "\n",
    "            rows.append({\n",
    "                'CardCode': card, 'dt': dt_t,\n",
    "                'momento_queda': momento_queda, 'tend_rel_inv': tend_rel_inv, 'recencia_n': recencia_n,\n",
    "                'coef_var': coef_var, 'tempo_ativo_meses': tempo_ativo_meses, 'queda_intensa': queda_intensa,\n",
    "                'peso_norm': map_peso_norm.get(card, 0.0),\n",
    "                'nps_risco_fill': map_nps_risco.get(card, NEUTRO_NPS),\n",
    "                'engaj_score': map_engaj.get(card, 0.0)\n",
    "            })\n",
    "\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    df_pred = pd.DataFrame(rows).sort_values(['dt','CardCode'])\n",
    "\n",
    "    \n",
    "    def norm_mes(gx):\n",
    "        arr = gx[['momento_queda','tend_rel_inv']].to_numpy().astype(float)\n",
    "        if arr.shape[0] > 1 and np.any(np.ptp(arr, axis=0) > 0):\n",
    "            sc = MinMaxScaler()\n",
    "            norm = sc.fit_transform(arr)\n",
    "            gx['momento_queda_n'] = norm[:,0]\n",
    "            gx['tend_rel_inv_n']  = norm[:,1]\n",
    "        else:\n",
    "            gx['momento_queda_n'] = 0.0\n",
    "            gx['tend_rel_inv_n']  = 0.0\n",
    "\n",
    "        gx['momento_queda_n_b'] = np.power(gx['momento_queda_n'], GAMMA)\n",
    "        gx['tend_rel_inv_n_b']  = np.power(gx['tend_rel_inv_n'],  GAMMA)\n",
    "        gx['recencia_n_b']      = np.power(gx['recencia_n'],      GAMMA)\n",
    "\n",
    "        gx['risco_raw'] = (\n",
    "            W_MOMENTO*gx['momento_queda_n_b'] +\n",
    "            W_TEND   *gx['tend_rel_inv_n_b']  +\n",
    "            W_REC    *gx['recencia_n_b']\n",
    "        ).clip(0,1)\n",
    "\n",
    "        rank_pct = gx['risco_raw'].rank(pct=True)\n",
    "        rank_pct_boost = np.power(rank_pct, 0.85)\n",
    "        gx['risco_blend'] = (ALPHA*gx['risco_raw'] + (1-ALPHA)*rank_pct_boost).clip(0,1)\n",
    "        gx['risco_score'] = (gx['risco_blend'] + INFLUENCIA_TICKET*gx['peso_norm']).clip(0,1)\n",
    "        return gx\n",
    "\n",
    "    df_pred = df_pred.groupby('dt', group_keys=False).apply(norm_mes)\n",
    "\n",
    "    \n",
    "    def label_real(card, dt_t):\n",
    "        dt_cancel = map_dt_cancel.get(card, pd.NaT)\n",
    "        if pd.isna(dt_cancel):\n",
    "            return 0\n",
    "        dt_limite = (dt_t + pd.DateOffset(months=K_PRAZO_MESES)).replace(day=1) + pd.offsets.MonthEnd(0)\n",
    "        return int((dt_cancel > dt_t) and (dt_cancel <= dt_limite))\n",
    "\n",
    "    df_pred['churn_label'] = [label_real(c, d) for c, d in zip(df_pred['CardCode'], df_pred['dt'])]\n",
    "\n",
    "   \n",
    "    features = [\n",
    "        'momento_queda','tend_rel_inv','recencia_n','peso_norm',\n",
    "        'coef_var','tempo_ativo_meses','queda_intensa','nps_risco_fill','engaj_score'\n",
    "    ]\n",
    "    df_train = df_pred.dropna(subset=features).copy()\n",
    "    if 'churn_label' not in df_train.columns or df_train['churn_label'].nunique() < 2:\n",
    "        return None\n",
    "\n",
    "    \n",
    "    ult_dt = df_train['dt'].max()\n",
    "    cut_dt = (ult_dt - pd.DateOffset(months=3)).normalize()\n",
    "\n",
    "    X_tr = df_train.loc[df_train['dt'] <= cut_dt, features].astype(float).to_numpy()\n",
    "    y_tr = df_train.loc[df_train['dt'] <= cut_dt, 'churn_label'].astype(int).to_numpy()\n",
    "    X_te = df_train.loc[df_train['dt']  > cut_dt, features].astype(float).to_numpy()\n",
    "    y_te = df_train.loc[df_train['dt']  > cut_dt, 'churn_label'].astype(int).to_numpy()\n",
    "\n",
    "    \n",
    "    if len(np.unique(y_tr)) < 2 or len(np.unique(y_te)) < 2:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_all = df_train[features].astype(float).to_numpy()\n",
    "        y_all = df_train['churn_label'].astype(int).to_numpy()\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X_all, y_all, test_size=0.25, random_state=42, stratify=y_all)\n",
    "\n",
    "    \n",
    "    used_smote = False\n",
    "    try:\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        sm = SMOTE(random_state=42, k_neighbors=5)\n",
    "        X_tr_res, y_tr_res = sm.fit_resample(X_tr, y_tr)\n",
    "        used_smote = True\n",
    "    except Exception:\n",
    "        X_tr_res, y_tr_res = X_tr, y_tr  # fallback sem SMOTE\n",
    "\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value=0.0)),  # <<< NOVO\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=500, solver='lbfgs'))\n",
    "    ])\n",
    "    pipe.fit(X_tr_res, y_tr_res)\n",
    "\n",
    "    \n",
    "    proba_te = pipe.predict_proba(X_te)[:,1]\n",
    "   \n",
    "    thr_opt, df_thr = pick_threshold(proba_te, y_te, thr_grid=THR_GRID, target_recall=TARGET_RECALL, beta=2.0)\n",
    "\n",
    "    \n",
    "    pipe.thr_opt_ = float(thr_opt)\n",
    "    pipe.feature_names_ = features\n",
    "    pipe.df_thr_cv_ = df_thr\n",
    "    pipe.used_smote_ = used_smote\n",
    "    return pipe\n",
    "\n",
    "# 8) Pipeline principal\n",
    "\n",
    "def main():\n",
    "    # Leitura\n",
    "    df_raw = ler_base_unificada(pasta)\n",
    "    df_panel = painel_mensal(df_raw)\n",
    "\n",
    "    if not isinstance(df_panel, pd.DataFrame) or df_panel.empty:\n",
    "        raise ValueError(\"df_panel não foi gerado corretamente pelo painel_mensal().\")\n",
    "\n",
    "    dfc = resumo_clientes(df_raw)\n",
    "\n",
    "    \n",
    "    df_ticket = ler_ticket(pasta)\n",
    "    df_nps    = ler_nps(pasta)\n",
    "\n",
    "    \n",
    "    df_regra = score_regra(df_panel, dfc, df_ticket, df_nps)\n",
    "\n",
    "    \n",
    "    pipe = tentar_treinar_modelo(df_panel, df_regra, pasta)\n",
    "\n",
    "    \n",
    "    if pipe is not None:\n",
    "        feats = getattr(pipe, 'feature_names_', [\n",
    "            'momento_queda','tend_rel_inv','recencia_n','peso_norm',\n",
    "            'coef_var','tempo_ativo_meses','queda_intensa','nps_risco_fill','engaj_score'\n",
    "        ])\n",
    "        \n",
    "        for f in feats:\n",
    "            if f not in df_regra.columns:\n",
    "                if f == 'nps_risco_fill':\n",
    "                    df_regra[f] = df_regra.get('risco_nps', pd.Series(np.nan)).fillna(NEUTRO_NPS)\n",
    "                elif f == 'engaj_score':\n",
    "                    df_regra[f] = df_regra.get('engajamento_score', pd.Series(0.0)).fillna(0.0)\n",
    "                else:\n",
    "                    df_regra[f] = 0.0\n",
    "\n",
    "        \n",
    "        df_regra[feats] = (\n",
    "            df_regra[feats]\n",
    "                .assign(\n",
    "                    nps_risco_fill=lambda x: x['nps_risco_fill'].fillna(NEUTRO_NPS) if 'nps_risco_fill' in x else NEUTRO_NPS,\n",
    "                    engaj_score=lambda x: x['engaj_score'].fillna(0.0)               if 'engaj_score'    in x else 0.0\n",
    "                )\n",
    "                .fillna(0.0)\n",
    "        )\n",
    "\n",
    "        \n",
    "        nan_counts = df_regra[feats].isna().sum().sort_values(ascending=False)\n",
    "        if nan_counts.sum() > 0:\n",
    "            print(\"\\n⚠️ Existem NaNs nas features ao vivo:\")\n",
    "            print(nan_counts[nan_counts > 0].to_string())\n",
    "\n",
    "        X_live = df_regra[feats].to_numpy(dtype=float)\n",
    "        probas = pipe.predict_proba(X_live)[:,1].clip(0,1)\n",
    "        df_regra['risco_ml'] = probas\n",
    "\n",
    "        thr = float(getattr(pipe, 'thr_opt_', 0.5))\n",
    "        df_regra['nivel_risco_ml'] = df_regra['risco_ml'].apply(\n",
    "            lambda p: \"Alto Risco\" if p >= thr else (\"Risco Médio\" if p >= 0.3 else \"Baixo Risco\")\n",
    "        )\n",
    "\n",
    "        \n",
    "        base_col = 'risco_churn_final_regra'\n",
    "        df_regra['comparativo_dif'] = df_regra['risco_ml'] - df_regra[base_col]\n",
    "    else:\n",
    "       \n",
    "        df_regra['risco_ml'] = np.nan\n",
    "        df_regra['nivel_risco_ml'] = np.nan\n",
    "        df_regra['comparativo_dif'] = np.nan\n",
    "\n",
    "    \n",
    "    cols_final = [\n",
    "        'CardCode',\n",
    "        'risco_churn_final_regra','nivel_risco_regra','categoria_churn_regra',\n",
    "        'engajamento_score','peso_norm','nota_nps','risco_nps',\n",
    "        'coef_var','tempo_ativo_meses','queda_intensa','nps_risco_fill','engaj_score',\n",
    "        'risco_ml','nivel_risco_ml','comparativo_dif'\n",
    "    ]\n",
    "    cols_final = [c for c in cols_final if c in df_regra.columns]\n",
    "    df_out = df_regra[cols_final].copy()\n",
    "\n",
    "    # Salvar apenas o resultado final\n",
    "    saida_csv_final = os.path.join(pasta, \"clientes_risco_final.csv\")\n",
    "    df_out.to_csv(saida_csv_final, index=False, encoding='utf-8')\n",
    "    print(f\"✅ Arquivo final salvo em: {saida_csv_final}\")\n",
    "    print(f\"📦 Linhas: {len(df_out):,} | Colunas: {len(df_out.columns)}\")\n",
    "\n",
    "   -    if pipe is not None:\n",
    "        thr = float(pipe.thr_opt_)\n",
    "        used_smote = getattr(pipe, \"used_smote_\", False)\n",
    "        print(\"\\n🔧 ML habilitado\")\n",
    "        print(f\"• SMOTE usado? {'Sim' if used_smote else 'Não'}\")\n",
    "        print(f\"• Threshold ótimo (prioriza recall / F2): {thr:.3f}\")\n",
    "        print(\"• Top thresholds (holdout):\")\n",
    "        print(pipe.df_thr_cv_.head(8).to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# 9) Avaliação pós-processamento\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "   \n",
    "    df_pred = pd.read_csv(os.path.join(pasta, \"clientes_risco_final.csv\"))\n",
    "\n",
    "   \n",
    "    df_cancel = pd.read_excel(os.path.join(pasta, \"cancelados.xlsx\"))\n",
    "    df_cancel['CardCode'] = df_cancel['CardCode'].astype(str).str.strip()\n",
    "\n",
    "    \n",
    "    df_pred['CardCode'] = df_pred['CardCode'].astype(str).str.strip()\n",
    "    df_pred['cancelado'] = df_pred['CardCode'].isin(df_cancel['CardCode']).astype(int)\n",
    "\n",
    "    label_col = 'nivel_risco_ml' if df_pred['nivel_risco_ml'].notna().any() else 'nivel_risco_regra'\n",
    "    df_pred['prev_churn'] = df_pred[label_col].apply(lambda x: 1 if \"Alto\" in str(x) else 0)\n",
    "\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm = confusion_matrix(df_pred['cancelado'], df_pred['prev_churn'])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    acuracia = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precisao = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall   = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1       = 2 * (precisao * recall) / (precisao + recall + 1e-9)\n",
    "\n",
    "    print(\"\\n📊 Resultados (pós-threshold e pós-pipeline):\")\n",
    "    print(f\"Acurácia: {acuracia*100:.2f}%\")\n",
    "    print(f\"Precisão: {precisao*100:.2f}%\")\n",
    "    print(f\"Recall (sensibilidade): {recall*100:.2f}%\")\n",
    "    print(f\"F1-Score: {f1*100:.2f}%\")\n",
    "    print(\"\\nMatriz de Confusão:\")\n",
    "    print(f\"Verdadeiros Negativos: {tn}\")\n",
    "    print(f\"Falsos Positivos: {fp}\")\n",
    "    print(f\"Falsos Negativos: {fn}\")\n",
    "    print(f\"Verdadeiros Positivos: {tp}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nℹ️ Avaliação final não executada (talvez sem cancelados.xlsx ou outra falha): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb20b72-8232-40af-8367-caa915a4b50a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b7a9be-531a-49d7-874b-cc8f426e2882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ac585-e8fe-4859-b079-867f8994980f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
